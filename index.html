<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GSO: Software Optimization Benchmark for SWE-Agents</title>
    <meta name="description" content="GSO is a benchmark for evaluating language models' capabilities in developing high-performance software through challenging optimization tasks.">
    <link rel="stylesheet" href="assets/style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body>
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-brand">
                <a href="index.html" style="text-decoration: none; color: inherit;">
                    <h1>GSO</h1>
                </a>
            </div>
            <div class="nav-links">
                <a href="overview.html">Overview</a>
                <a href="tasks.html" class="nav-hide-mobile">Tasks</a>
                <a href="https://gso-blog.notion.site/" target="_blank" rel="noopener noreferrer" class="nav-hide-mobile">Blog</a>
                <a href="https://github.com/gso-bench/gso" target="_blank" rel="noopener noreferrer">GitHub</a>
                <a href="https://arxiv.org/pdf/2505.23671" target="_blank" rel="noopener noreferrer" class="nav-hide-mobile">Paper</a>
                <a href="https://huggingface.co/datasets/gso-bench/gso" target="_blank" rel="noopener noreferrer" class="nav-hide-mobile">Dataset</a>
                <button id="theme-toggle" class="theme-toggle" aria-label="Toggle dark mode">
                    <span class="theme-icon"></span>
                </button>
            </div>
        </div>
    </nav>

    <main>
        <!-- Hero with Overview -->
        <section class="hero-minimal">
            <div class="container">
                <h1 class="hero-title-main">GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents</h1>
                <p class="hero-authors">Manish Shetty, Naman Jain, Jinjian Liu, Vijay Kethanaboyina, Koushik Sen, Ion Stoica</p>
                
                <div class="hero-overview">
                    <p>In each task, an agent is given a codebase and a performance test as a <em>precise specification</em>, 
                    and must improve runtime efficiency to match an expert developer's optimization.</p>
                </div>
                
                <p class="hero-meta">102 tasks · 10 codebases · 5 languages · <a href="overview.html">Learn more →</a></p>
            </div>
        </section>

        <!-- Leaderboard -->
        <section class="section-clean">
            <div class="container">
                <div class="leaderboard-header">
                    <h2>Leaderboard</h2>
                    <div id="leaderboard-filters" class="leaderboard-filters-bar"></div>
                </div>
                
                <div class="leaderboard-container">
                    <div class="leaderboard-loading" id="leaderboard-loading">
                        <div class="spinner"></div>
                        <span>Loading leaderboard...</span>
                    </div>
                    <div class="leaderboard-table" id="leaderboard-table" style="display: none;"></div>
                </div>
                
                <div class="leaderboard-footnotes">
                    <p><strong>Opt@1:</strong> Estimator of fraction of tasks where a single attempt achieves ≥95% human speedup and passes correctness tests. <a href="https://arxiv.org/pdf/2505.23671" target="_blank">See paper for details</a>.</p>
                    <p><strong><span class="delta-negative">&nbsp;NEW&nbsp;</span>  Hack-Adjusted:</strong> 
                        We observe that top models can perform deceptive optimizations (e.g., memoization, harness hijacking, etc.).
                        To tackle this we introduce a new "Hack Detector" system that penalizes reward hacks by comparing the model's patch with the oracle solution and the test cases.
                        The "Hack-Adjusted" column shows the adjusted score after penalizing detected hacks.
                        <a href="https://gso-blog.notion.site/gso-hackdetector" target="_blank">Learn more</a>.
                    </p>
                </div>

                <!-- Opt@1 vs Speedup Threshold Plot -->
                <div class="plot-section">
                    <h2 style="text-align: left;">Opt@1 vs Speedup Threshold (p)</h2>
                    <div>
                        <p>
                            <strong>Opt<sub>p</sub>@1:</strong> Estimate of fraction of tasks where a single attempt achieves ≥p% human speedup and passes correctness tests.
                        </p>
                        <p>
                            p=0.95 is our default threshold, but it can be a knob for difficulty for a task; i.e., p=0 evaluates if the agent's patch is correct, regardless of performance, 
                            while p=1 evaluates if the agent's patch is identical/better in performance to the human commit.
                        </p>
                    </div>
                    <div class="plot-container">
                        <canvas id="opt1-threshold-plot"></canvas>
                    </div>
                </div>
            </div>
        </section>
    </main>

    <footer class="footer-minimal">
        <div class="container">
            <p>GSO Benchmark · <a href="mailto:manishs@berkeley.edu">Contact</a> · <a href="https://github.com/gso-bench/gso">GitHub</a></p>
        </div>
    </footer>

    <script src="assets/script.js"></script>
    <script src="assets/opt1-plot.js"></script>
</body>
</html>
